{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5ca4d4d",
   "metadata": {
    "id": "e5ca4d4d"
   },
   "source": [
    "## Fine tuning LlaMA 2\n",
    "This script demonstrate how one can fine tune Llama2 using qLoRA\n",
    "\n",
    "It is trained using the [Dolly datasets](https://huggingface.co/datasets/databricks/databricks-dolly-15k)\n",
    "\n",
    "It requires at least one GPU to be available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12944e29",
   "metadata": {
    "id": "12944e29"
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import dataclasses\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import typing as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "Z9sjGR4PD3EH",
   "metadata": {
    "id": "Z9sjGR4PD3EH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting peft@ git+https://github.com/huggingface/peft\n",
      "  Cloning https://github.com/huggingface/peft to /tmp/pip-install-ll71nxe5/peft_01ce3c32437446fbba8e86c60de6fdbf\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft /tmp/pip-install-ll71nxe5/peft_01ce3c32437446fbba8e86c60de6fdbf\n",
      "  Resolved https://github.com/huggingface/peft to commit e06d94ddeb6c70913593740618df76908b918d66\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: datasets[s3]==2.13.0 in ./venv/lib/python3.10/site-packages (2.13.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.10/site-packages (from datasets[s3]==2.13.0) (1.25.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in ./venv/lib/python3.10/site-packages (from datasets[s3]==2.13.0) (12.0.1)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in ./venv/lib/python3.10/site-packages (from datasets[s3]==2.13.0) (0.3.6)\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.10/site-packages (from datasets[s3]==2.13.0) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./venv/lib/python3.10/site-packages (from datasets[s3]==2.13.0) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in ./venv/lib/python3.10/site-packages (from datasets[s3]==2.13.0) (4.65.0)\n",
      "Requirement already satisfied: xxhash in ./venv/lib/python3.10/site-packages (from datasets[s3]==2.13.0) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in ./venv/lib/python3.10/site-packages (from datasets[s3]==2.13.0) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in ./venv/lib/python3.10/site-packages (from datasets[s3]==2.13.0) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in ./venv/lib/python3.10/site-packages (from datasets[s3]==2.13.0) (3.8.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in ./venv/lib/python3.10/site-packages (from datasets[s3]==2.13.0) (0.16.4)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.10/site-packages (from datasets[s3]==2.13.0) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.10/site-packages (from datasets[s3]==2.13.0) (6.0.1)\n",
      "Requirement already satisfied: s3fs in ./venv/lib/python3.10/site-packages (from datasets[s3]==2.13.0) (2023.6.0)\n",
      "Requirement already satisfied: psutil in ./venv/lib/python3.10/site-packages (from peft@ git+https://github.com/huggingface/peft) (5.9.5)\n",
      "Requirement already satisfied: torch>=1.13.0 in ./venv/lib/python3.10/site-packages (from peft@ git+https://github.com/huggingface/peft) (2.0.1)\n",
      "Requirement already satisfied: transformers in ./venv/lib/python3.10/site-packages (from peft@ git+https://github.com/huggingface/peft) (4.31.0)\n",
      "Requirement already satisfied: accelerate in ./venv/lib/python3.10/site-packages (from peft@ git+https://github.com/huggingface/peft) (0.21.0)\n",
      "Requirement already satisfied: safetensors in ./venv/lib/python3.10/site-packages (from peft@ git+https://github.com/huggingface/peft) (0.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.10/site-packages (from aiohttp->datasets[s3]==2.13.0) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in ./venv/lib/python3.10/site-packages (from aiohttp->datasets[s3]==2.13.0) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/lib/python3.10/site-packages (from aiohttp->datasets[s3]==2.13.0) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in ./venv/lib/python3.10/site-packages (from aiohttp->datasets[s3]==2.13.0) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./venv/lib/python3.10/site-packages (from aiohttp->datasets[s3]==2.13.0) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./venv/lib/python3.10/site-packages (from aiohttp->datasets[s3]==2.13.0) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./venv/lib/python3.10/site-packages (from aiohttp->datasets[s3]==2.13.0) (1.3.1)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets[s3]==2.13.0) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets[s3]==2.13.0) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.10/site-packages (from requests>=2.19.0->datasets[s3]==2.13.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests>=2.19.0->datasets[s3]==2.13.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.10/site-packages (from requests>=2.19.0->datasets[s3]==2.13.0) (2023.5.7)\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.10/site-packages (from torch>=1.13.0->peft@ git+https://github.com/huggingface/peft) (1.12)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.10/site-packages (from torch>=1.13.0->peft@ git+https://github.com/huggingface/peft) (3.1)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.10/site-packages (from torch>=1.13.0->peft@ git+https://github.com/huggingface/peft) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in ./venv/lib/python3.10/site-packages (from torch>=1.13.0->peft@ git+https://github.com/huggingface/peft) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in ./venv/lib/python3.10/site-packages (from torch>=1.13.0->peft@ git+https://github.com/huggingface/peft) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in ./venv/lib/python3.10/site-packages (from torch>=1.13.0->peft@ git+https://github.com/huggingface/peft) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in ./venv/lib/python3.10/site-packages (from torch>=1.13.0->peft@ git+https://github.com/huggingface/peft) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in ./venv/lib/python3.10/site-packages (from torch>=1.13.0->peft@ git+https://github.com/huggingface/peft) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in ./venv/lib/python3.10/site-packages (from torch>=1.13.0->peft@ git+https://github.com/huggingface/peft) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in ./venv/lib/python3.10/site-packages (from torch>=1.13.0->peft@ git+https://github.com/huggingface/peft) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in ./venv/lib/python3.10/site-packages (from torch>=1.13.0->peft@ git+https://github.com/huggingface/peft) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in ./venv/lib/python3.10/site-packages (from torch>=1.13.0->peft@ git+https://github.com/huggingface/peft) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in ./venv/lib/python3.10/site-packages (from torch>=1.13.0->peft@ git+https://github.com/huggingface/peft) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in ./venv/lib/python3.10/site-packages (from torch>=1.13.0->peft@ git+https://github.com/huggingface/peft) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in ./venv/lib/python3.10/site-packages (from torch>=1.13.0->peft@ git+https://github.com/huggingface/peft) (2.0.0)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.13.0->peft@ git+https://github.com/huggingface/peft) (59.6.0)\n",
      "Requirement already satisfied: wheel in ./venv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.13.0->peft@ git+https://github.com/huggingface/peft) (0.40.0)\n",
      "Requirement already satisfied: cmake in ./venv/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.13.0->peft@ git+https://github.com/huggingface/peft) (3.27.0)\n",
      "Requirement already satisfied: lit in ./venv/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.13.0->peft@ git+https://github.com/huggingface/peft) (16.0.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.10/site-packages (from pandas->datasets[s3]==2.13.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.10/site-packages (from pandas->datasets[s3]==2.13.0) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./venv/lib/python3.10/site-packages (from pandas->datasets[s3]==2.13.0) (2023.3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aiobotocore~=2.5.0 in ./venv/lib/python3.10/site-packages (from s3fs->datasets[s3]==2.13.0) (2.5.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.10/site-packages (from transformers->peft@ git+https://github.com/huggingface/peft) (2023.6.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./venv/lib/python3.10/site-packages (from transformers->peft@ git+https://github.com/huggingface/peft) (0.13.3)\n",
      "Requirement already satisfied: botocore<1.29.162,>=1.29.161 in ./venv/lib/python3.10/site-packages (from aiobotocore~=2.5.0->s3fs->datasets[s3]==2.13.0) (1.29.161)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in ./venv/lib/python3.10/site-packages (from aiobotocore~=2.5.0->s3fs->datasets[s3]==2.13.0) (1.15.0)\n",
      "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in ./venv/lib/python3.10/site-packages (from aiobotocore~=2.5.0->s3fs->datasets[s3]==2.13.0) (0.11.0)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets[s3]==2.13.0) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft@ git+https://github.com/huggingface/peft) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./venv/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft@ git+https://github.com/huggingface/peft) (1.3.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in ./venv/lib/python3.10/site-packages (from botocore<1.29.162,>=1.29.161->aiobotocore~=2.5.0->s3fs->datasets[s3]==2.13.0) (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"peft@git+https://github.com/huggingface/peft\" \"datasets[s3]==2.13.0\"\n",
    "# !pip install -U \"openllm[fine-tune]@git+https://github.com/bentoml/OpenLLM.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc641c90",
   "metadata": {
    "id": "cc641c90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin /workspace/notebooks/venv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n"
     ]
    }
   ],
   "source": [
    "# import openllm here for OPENLLMDEVDEBUG\n",
    "import openllm\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "if t.TYPE_CHECKING:\n",
    "    import peft\n",
    "\n",
    "openllm.utils.configure_logging()\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "from datasets import load_dataset\n",
    "from random import randint\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "from random import randrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fb55fe2",
   "metadata": {
    "id": "5fb55fe2"
   },
   "outputs": [],
   "source": [
    "# COPIED FROM https://github.com/artidoro/qlora/blob/main/qlora.py\n",
    "def find_all_linear_names(model):\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, bnb.nn.Linear4bit):\n",
    "            names = name.split(\".\")\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if \"lm_head\" in lora_module_names:  # needed for 16-bit\n",
    "        lora_module_names.remove(\"lm_head\")\n",
    "    return list(lora_module_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8075f626",
   "metadata": {
    "id": "8075f626",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Change this to the local converted path if you don't have access to the meta-llama model\n",
    "DEFAULT_MODEL_ID = \"meta-llama/Llama-2-7b-hf\"\n",
    "DATASET_NAME = \"databricks/databricks-dolly-15k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba71d65e",
   "metadata": {
    "id": "ba71d65e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 'meta-llama/Llama-2-7b-hf' to lowercase: 'meta-llama/llama-2-7b-hf'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting 'meta-llama/Llama-2-7b-hf' to lowercase: 'meta-llama/llama-2-7b-hf'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 'meta-llama/Llama-2-7b-hf' to lowercase: 'meta-llama/llama-2-7b-hf'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting 'meta-llama/Llama-2-7b-hf' to lowercase: 'meta-llama/llama-2-7b-hf'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m__tag__:pt-meta-llama-llama-2-7b-hf:335a02887eb6684d487240bbc28b5699298c3135\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(tag=\"pt-meta-llama-llama-2-7b-hf:335a02887eb6684d487240bbc28b5699298c3135\", path=\"/home/larme/bentoml/models/pt-meta-llama-llama-2-7b-hf/335a02887eb6684d487240bbc28b5699298c3135\")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the model first hand\n",
    "openllm.import_model(\"llama\", model_id=DEFAULT_MODEL_ID, model_version=\"335a02887eb6684d487240bbc28b5699298c3135\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6af020c",
   "metadata": {
    "id": "d6af020c"
   },
   "outputs": [],
   "source": [
    "def format_dolly(sample):\n",
    "    instruction = f\"### Instruction\\n{sample['instruction']}\"\n",
    "    context = f\"### Context\\n{sample['context']}\" if len(sample[\"context\"]) > 0 else None\n",
    "    response = f\"### Answer\\n{sample['response']}\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93539c9d",
   "metadata": {
    "id": "93539c9d"
   },
   "outputs": [],
   "source": [
    "# template dataset to add prompt to each sample\n",
    "def template_dataset(sample, tokenizer):\n",
    "    sample[\"text\"] = f\"{format_dolly(sample)}{tokenizer.eos_token}\"\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42d99965",
   "metadata": {
    "id": "42d99965"
   },
   "outputs": [],
   "source": [
    "# empty list to save remainder from batches to use in next batch\n",
    "remainder = {\"input_ids\": [], \"attention_mask\": [], \"token_type_ids\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2437005",
   "metadata": {
    "id": "b2437005"
   },
   "outputs": [],
   "source": [
    "def chunk(sample, chunk_length=2048):\n",
    "    # define global remainder variable to save remainder from batches to use in next batch\n",
    "    global remainder\n",
    "    # Concatenate all texts and add remainder from previous batch\n",
    "    concatenated_examples = {k: list(chain(*sample[k])) for k in sample.keys()}\n",
    "    concatenated_examples = {k: remainder[k] + concatenated_examples[k] for k in concatenated_examples.keys()}\n",
    "    # get total number of tokens for batch\n",
    "    batch_total_length = len(concatenated_examples[list(sample.keys())[0]])\n",
    "\n",
    "    # get max number of chunks for batch\n",
    "    if batch_total_length >= chunk_length:\n",
    "        batch_chunk_length = (batch_total_length // chunk_length) * chunk_length\n",
    "\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_length] for i in range(0, batch_chunk_length, chunk_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # add remainder to global variable for next batch\n",
    "    remainder = {k: concatenated_examples[k][batch_chunk_length:] for k in concatenated_examples.keys()}\n",
    "    # prepare labels\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96ed2cd2",
   "metadata": {
    "id": "96ed2cd2"
   },
   "outputs": [],
   "source": [
    "def prepare_datasets(tokenizer, dataset_name=DATASET_NAME):\n",
    "    # Load dataset from the hub\n",
    "    dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "    print(f\"dataset size: {len(dataset)}\")\n",
    "    print(dataset[randrange(len(dataset))])\n",
    "\n",
    "    # apply prompt template per sample\n",
    "    dataset = dataset.map(partial(template_dataset, tokenizer=tokenizer), remove_columns=list(dataset.features))\n",
    "    # print random sample\n",
    "    print(\"Sample from dolly-v2 ds:\", dataset[randint(0, len(dataset))][\"text\"])\n",
    "\n",
    "    # tokenize and chunk dataset\n",
    "    lm_dataset = dataset.map(\n",
    "        lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(dataset.features)\n",
    "    ).map(\n",
    "        partial(chunk, chunk_length=2048),\n",
    "        batched=True,\n",
    "    )\n",
    "\n",
    "    # Print total number of samples\n",
    "    print(f\"Total number of samples: {len(lm_dataset)}\")\n",
    "    return lm_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a506d91",
   "metadata": {
    "id": "9a506d91"
   },
   "outputs": [],
   "source": [
    "@openllm.utils.requires_dependencies(\"peft\", extra=\"fine-tune\")\n",
    "def prepare_for_int4_training(\n",
    "    model_id: str, gradient_checkpointing: bool = True, bf16: bool = True\n",
    ") -> tuple[peft.PeftModel, transformers.LlamaTokenizerFast]:\n",
    "    from peft.tuners.lora import LoraLayer\n",
    "\n",
    "    llm = openllm.AutoLLM.for_model(\n",
    "        \"llama\",\n",
    "        model_id=model_id,\n",
    "        model_version=\"335a02887eb6684d487240bbc28b5699298c3135\",\n",
    "        ensure_available=True,\n",
    "        quantize=\"int4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        use_cache=not gradient_checkpointing,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    print(\"Model summary:\", llm.model)\n",
    "\n",
    "    # get lora target modules\n",
    "    modules = find_all_linear_names(llm.model)\n",
    "    print(f\"Found {len(modules)} modules to quantize: {modules}\")\n",
    "\n",
    "    model, tokenizer = llm.prepare_for_training(adapter_type=\"lora\", use_gradient_checkpointing=gradient_checkpointing)\n",
    "\n",
    "    # pre-process the model by upcasting the layer norms in float 32 for\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, LoraLayer):\n",
    "            if bf16:\n",
    "                module = module.to(torch.bfloat16)\n",
    "        if \"norm\" in name:\n",
    "            module = module.to(torch.float32)\n",
    "        if \"lm_head\" in name or \"embed_tokens\" in name:\n",
    "            if hasattr(module, \"weight\"):\n",
    "                if bf16 and module.weight.dtype == torch.float32:\n",
    "                    module = module.to(torch.bfloat16)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f0c2e2a",
   "metadata": {
    "id": "4f0c2e2a"
   },
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class TrainingArguments:\n",
    "    per_device_train_batch_size: int = dataclasses.field(default=1)\n",
    "    gradient_checkpointing: bool = dataclasses.field(default=True)\n",
    "    bf16: bool = dataclasses.field(default=torch.cuda.get_device_capability()[0] == 8)\n",
    "    learning_rate: float = dataclasses.field(default=5e-5)\n",
    "    num_train_epochs: int = dataclasses.field(default=3)\n",
    "    logging_steps: int = dataclasses.field(default=1)\n",
    "    logging_strategy: str = dataclasses.field(default=\"steps\")\n",
    "    output_dir: str = dataclasses.field(default=os.path.join(os.getcwd(), \"outputs\", \"llama\"))\n",
    "    save_strategy: str = dataclasses.field(default=\"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d60ac11",
   "metadata": {
    "id": "4d60ac11"
   },
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class ModelArguments:\n",
    "    model_id: str = dataclasses.field(default=DEFAULT_MODEL_ID)\n",
    "    seed: int = dataclasses.field(default=42)\n",
    "    merge_weights: bool = dataclasses.field(default=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "286ccbf0",
   "metadata": {
    "id": "286ccbf0"
   },
   "outputs": [],
   "source": [
    "if openllm.utils.in_notebook():\n",
    "    model_args, training_args = ModelArguments(), TrainingArguments()\n",
    "else:\n",
    "    parser = transformers.HfArgumentParser((ModelArguments, TrainingArguments))\n",
    "    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n",
    "        # If we pass only one argument to the script and it's the path to a json file,\n",
    "        # let's parse it to get our arguments.\n",
    "        model_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n",
    "    else:\n",
    "        model_args, training_args = t.cast(\n",
    "            t.Tuple[ModelArguments, TrainingArguments], parser.parse_args_into_dataclasses()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7d29afc",
   "metadata": {
    "id": "f7d29afc"
   },
   "outputs": [],
   "source": [
    "def train_loop(model_args: ModelArguments, training_args: TrainingArguments):\n",
    "    import peft\n",
    "\n",
    "    transformers.set_seed(model_args.seed)\n",
    "\n",
    "    model, tokenizer = prepare_for_int4_training(\n",
    "        model_args.model_id,\n",
    "        gradient_checkpointing=training_args.gradient_checkpointing,\n",
    "        bf16=training_args.bf16,\n",
    "    )\n",
    "    datasets = prepare_datasets(tokenizer)\n",
    "\n",
    "    trainer = transformers.Trainer(\n",
    "        model=model,\n",
    "        args=dataclasses.replace(\n",
    "            transformers.TrainingArguments(training_args.output_dir), **dataclasses.asdict(training_args)\n",
    "        ),\n",
    "        train_dataset=datasets,\n",
    "        data_collator=transformers.default_data_collator,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    if model_args.merge_weights:\n",
    "        # note that this will requires larger GPU as we will load the whole model into memory\n",
    "\n",
    "        # merge adapter weights with base model and save\n",
    "        # save int4 model\n",
    "        trainer.model.save_pretrained(training_args.output_dir, safe_serialization=False)\n",
    "\n",
    "        # gc mem\n",
    "        del model, trainer\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        model = peft.AutoPeftModelForCausalLM.from_pretrained(\n",
    "            training_args.output_dir, low_cpu_mem_usage=True, torch_dtype=torch.float16\n",
    "        )\n",
    "        # merge lora with base weights and save\n",
    "        model = model.merge_and_unload()\n",
    "        model.save_pretrained(\n",
    "            os.path.join(os.getcwd(), \"outputs\", \"merged_llama_lora\"), safe_serialization=True, max_shard_size=\"2GB\"\n",
    "        )\n",
    "    else:\n",
    "        trainer.model.save_pretrained(os.path.join(training_args.output_dir, \"lora\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "306d8e9f",
   "metadata": {
    "id": "306d8e9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-23 15:45:53,107] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Converting 'meta-llama/Llama-2-7b-hf' to lowercase: 'meta-llama/llama-2-7b-hf'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting 'meta-llama/Llama-2-7b-hf' to lowercase: 'meta-llama/llama-2-7b-hf'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 'meta-llama/Llama-2-7b-hf' to lowercase: 'meta-llama/llama-2-7b-hf'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting 'meta-llama/Llama-2-7b-hf' to lowercase: 'meta-llama/llama-2-7b-hf'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 'meta-llama/Llama-2-7b-hf' to lowercase: 'meta-llama/llama-2-7b-hf'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting 'meta-llama/Llama-2-7b-hf' to lowercase: 'meta-llama/llama-2-7b-hf'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 'meta-llama/Llama-2-7b-hf' to lowercase: 'meta-llama/llama-2-7b-hf'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting 'meta-llama/Llama-2-7b-hf' to lowercase: 'meta-llama/llama-2-7b-hf'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m__tag__:pt-meta-llama-llama-2-7b-hf:335a02887eb6684d487240bbc28b5699298c3135\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10e77d76474e49c685228058b921839a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary: LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n",
      "Found 7 modules to quantize: ['o_proj', 'gate_proj', 'k_proj', 'down_proj', 'up_proj', 'q_proj', 'v_proj']\n",
      "dataset size: 15011\n",
      "{'instruction': \"What athlete created the 'beast quake' for the Seattle Seahawks?\", 'context': '', 'response': 'Marshan Lynch', 'category': 'open_qa'}\n",
      "Sample from dolly-v2 ds: ### Instruction\n",
      "Who wrote Democracy in America?\n",
      "\n",
      "### Answer\n",
      "Alexis de Tocqueville wrote Democracy in America</s>\n",
      "Total number of samples: 1581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/venv/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='4743' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   8/4743 00:08 < 1:47:42, 0.73 it/s, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.868900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.691800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.736800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.240100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.755400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.010700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 22\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(model_args, training_args)\u001b[0m\n\u001b[1;32m     11\u001b[0m datasets \u001b[38;5;241m=\u001b[39m prepare_datasets(tokenizer)\n\u001b[1;32m     13\u001b[0m trainer \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m     14\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     15\u001b[0m     args\u001b[38;5;241m=\u001b[39mdataclasses\u001b[38;5;241m.\u001b[39mreplace(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mtransformers\u001b[38;5;241m.\u001b[39mdefault_data_collator,\n\u001b[1;32m     20\u001b[0m )\n\u001b[0;32m---> 22\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_args\u001b[38;5;241m.\u001b[39mmerge_weights:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# note that this will requires larger GPU as we will load the whole model into memory\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# merge adapter weights with base model and save\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# save int4 model\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave_pretrained(training_args\u001b[38;5;241m.\u001b[39moutput_dir, safe_serialization\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/workspace/notebooks/venv/lib/python3.10/site-packages/transformers/trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1536\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1537\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1538\u001b[0m )\n\u001b[0;32m-> 1539\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/notebooks/venv/lib/python3.10/site-packages/transformers/trainer.py:1814\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1808\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   1809\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1812\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1813\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m-> 1814\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1815\u001b[0m ):\n\u001b[1;32m   1816\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1817\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   1818\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loop(model_args, training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a9bbb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
